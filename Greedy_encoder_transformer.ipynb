{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPRGnvxbijMJZRnmTUEZU5o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimdonggyu2008/generative-ai-python/blob/main/Greedy_encoder_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdata torchtext portalocker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l841G8kZs8qC",
        "outputId": "87c9f3db-e922-4f89-c26b-d55b50ce65f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchdata\n",
            "  Downloading torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.31.0)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->torchdata) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2->torchdata) (1.3.0)\n",
            "Installing collected packages: portalocker, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchdata\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 portalocker-2.8.2 torchdata-0.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "ygyMayCLszFs",
        "outputId": "2aedf422-ce12-45f6-e238-75da7512a2bb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "[E050] Can't find model 'de_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-aeb841070850>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m token_transform={\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mSRC_LANGUAGE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mget_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spacy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"de_core_news_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mTGT_LANGUAGE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mget_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spacy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"en_core_web_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m }\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchtext/data/utils.py\u001b[0m in \u001b[0;36mget_tokenizer\u001b[0;34m(tokenizer, language)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mspacy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0;31m# Model shortcuts no longer work in spaCy 3.0+, try using fullnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \"\"\"\n\u001b[0;32m---> 51\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'de_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
          ]
        }
      ],
      "source": [
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "def generate_tokens(text_iter,language):\n",
        "  language_index={SRC_LANGUAGE:0, TGT_LANGUAGE:1}\n",
        "\n",
        "  for text in text_iter:\n",
        "    yield token_transformer[language](text[language_index[language]])\n",
        "\n",
        "SRC_LANGUAGE=\"de\"\n",
        "TGT_LANGUAGE=\"en\"\n",
        "UNK_IDX,PAD_IDX,BOS_IDX,EOS_IDX=0,1,2,3\n",
        "special_symbols=[\"<unk>\",\"<pad>\",\"<bos>\",\"<eos>\"]\n",
        "\n",
        "token_transform={\n",
        "    SRC_LANGUAGE:get_tokenizer(\"spacy\",language=\"de_core_news_sm\"),\n",
        "    TGT_LANGUAGE:get_tokenizer(\"spacy\",language=\"en_core_web_sm\"),\n",
        "}\n",
        "print(\"Token Transform:\")\n",
        "print(token_transform)\n",
        "\n",
        "\n",
        "vocab_transform={}\n",
        "for language in [SRC_LANGUAGE,TGT_LANGUAGE]:\n",
        "  train_iter=Multi30k(split=\"train\",language_pair=(SRC_LANGUAGE,TGT_LANGUAGE))\n",
        "  vocab_transform[language]=build_vocab_from_iterator(\n",
        "      generate_tokens(train_iter,language),\n",
        "      min_freq=1,\n",
        "      specials=special_symbols,\n",
        "      special_first=True,\n",
        "  )\n",
        "\n",
        "for language in [SRC_LANGUAGE,TGT_LANGUAGE]:\n",
        "  vocab_transform[language].set_default_index(UNK_IDX)\n",
        "\n",
        "print(\"vocab transform\")\n",
        "print(vocab_transform)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self,d_model,max_len,dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.dropout=nn.Module(p=dropout)\n",
        "\n",
        "    positional=torch.arange(max_len).unsqueeze(1)\n",
        "    div_term=torch.exp(\n",
        "        torch.aragne(0,d_model,2)*(-math.log(10000.0)/d_model)\n",
        "    )\n",
        "\n",
        "    pe.torch.zeros(max_len,1,d_model)\n",
        "    pe[:,0,0::2]=torch.sin(position*div_term)\n",
        "    pe[:,0,1::2]=torch.cos(position*div_term)\n",
        "    self.register_buffer(\"pe\",pe)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=x+self.pe[:x.size(0)]\n",
        "    return self.dropout(x)\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "  def __init__(self,vocab_size,emb_size):\n",
        "    super().__init__()\n",
        "    self.embedding=nn.Embedding(vocab_size,emb_size)\n",
        "    self.emb_size=emb_size\n",
        "\n",
        "  def forward(self,tokens):\n",
        "    return self.embedding(tokens.long())*math.sqrt(self.emb_size)\n",
        "\n",
        "\n",
        "class Seq2SeqTransformater(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      num_encoder_layers,\n",
        "      num_decoder_layers,\n",
        "      emb_size,\n",
        "      max_len,\n",
        "      nhead,\n",
        "      src_vocab_size,\n",
        "      tgt_vocab_size,\n",
        "      dim_feedforward,\n",
        "      dropout=0.1\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.src_tok_emb=TokenEmbedding(src_vocab_size,emb_size)\n",
        "    self.tgt_tok_emb=TokenEmbedding(tgt_vocab_size,emb_size)\n",
        "    self.positional_encoding=PositionalEncoding(\n",
        "        d_model=emb_size,max_len=max_len,dropout=dropout\n",
        "    )\n",
        "\n",
        "    self.transformer=nn.Transformer(\n",
        "        d_model=emb_size,\n",
        "        nhead=nhead,\n",
        "        num_encoder_layers=num_encoder_layers,\n",
        "        num_decoder_layers=num_decoder_layers,\n",
        "        dim_feedforward=dim_feedforward,\n",
        "        dropout=dropout\n",
        "    )\n",
        "    self.generator=nn.Linear(emb_size,tgt_vocab_size)\n",
        "\n",
        "  def forward(\n",
        "    self,\n",
        "    src,\n",
        "    trg,\n",
        "    src_mask,\n",
        "    tgt_mask,\n",
        "    src_padding_mask,\n",
        "    tgt_padding_mask,\n",
        "    memory_key_padding_mask,\n",
        "  ):\n",
        "    src_emb=self.positional_encoding(self.src_tok_emb(src))\n",
        "    tgt_emb=self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "    outs=self.transformer(\n",
        "        src=src_emb,\n",
        "        tgt=tgt_emb,\n",
        "        src_mask=src_mask,\n",
        "        memory_mask=None,\n",
        "        src_key_padding_mask=src_padding_mask,\n",
        "        tgt_key_padding_mask=memory_key_padding_mask\n",
        "\n",
        "    )\n",
        "    return self.generator(outs)\n",
        "\n",
        "  def encode(self,src,src_mask):\n",
        "    return self.transformer.encoder(\n",
        "        self.positional_encoding(self.src_tok_emb(src)),src_mask\n",
        "    )\n",
        "\n",
        "  def decode(self,tgt,memory,tgt_mask):\n",
        "    return self.transformer.decoder(\n",
        "        self.positional_encoding(self.tgt_tok_emb(tgt)),memory,tgt_mask\n",
        "    )"
      ],
      "metadata": {
        "id": "wp9lJWlivnfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer=torch.nn.Transformer(\n",
        "    d_model=512,\n",
        "    nhead=8,\n",
        "    num_encoder_layers=6,\n",
        "    num_decoder_layers=6,\n",
        "    dim_feedforward=2048,\n",
        "    dropout=0.1,\n",
        "    activation=torch.nn.functional.relu,\n",
        "    layer_norm_eps=1e-05,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdhRW9gL2tZp",
        "outputId": "50bac2a5-8ae8-40c9-8c0a-3739f6ef7e3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def sequential_transformers(*transformers):\n",
        "  def func(txt_input):\n",
        "    for tranform in transforms:\n",
        "      txt_input=transform(txt_input)\n",
        "    return txt_input\n",
        "  return func\n",
        "\n",
        "def input_transform(token_ids):\n",
        "  return torch.cat(\n",
        "      (torch.tensor([BOS_IDX]),torch.tensor(token_ids),torch.tensor([EOS_IDX]))\n",
        "  )\n",
        "\n",
        "def collator(batch):\n",
        "  src_batch,tgt_batch=[],[]\n",
        "  for src_sample,tgt_sample in batch:\n",
        "    src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "    tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "  src_batch=pad_sequence(src_batch,padding_value=PAD_IDX)\n",
        "  tgt_batch=pad_sequence(tgt_batch,padding_value=PAD_IDX)\n",
        "  return src_batch,tgt_batch\n",
        "\n",
        "text_transform={}\n",
        "for language in [SRC_LANGUAGE,TGT_LANGUAGE]:\n",
        "  text_transform[language]=sequential_transforms(\n",
        "      token_transform[language],vocab_transform[language],input_transform\n",
        "  )\n",
        "\n",
        "data_iter=Multi30k(split=\"valid\",language_pair(SRC_LANGUAGE,TGT_LANGUAGE))\n",
        "dataloader=DataLoader(data_iter,batch_size=BATCH_SIZE,collate_fn=collator)\n",
        "source_tensor,target_tensor=next(iter(dataloader))\n",
        "\n",
        "print(\"(source,target):\")\n",
        "print(next(iter(data_iter)))\n",
        "\n",
        "print(\"soruce_batch:\", source_tensor.shape)\n",
        "print(source_tensor)\n",
        "\n",
        "print(\"target_batch:\",target_tensor.shape)\n",
        "print(target_tensor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "DRPbHcQS36Jf",
        "outputId": "e79ff42b-4ee9-4b1d-f166-98d44be69048"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "positional argument follows keyword argument (<ipython-input-10-2b9f76a9a59a>, line 31)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-2b9f76a9a59a>\"\u001b[0;36m, line \u001b[0;32m31\u001b[0m\n\u001b[0;31m    data_iter=Multi30k(split=\"valid\",language_pair(SRC_LANGUAGE,TGT_LANGUAGE))\u001b[0m\n\u001b[0m                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_square_subsequent_mask(s):\n",
        "  mask=(torch.triu(torch.ones((s,s),device=DEVICE))==1).transpose(0,1)\n",
        "  mask=(\n",
        "      mask.float()\n",
        "      .masked_fill(mask==0,float(\"-inf\"))\n",
        "      .masked_fill(mask==1,float(0,0))\n",
        "  )\n",
        "  return mask\n",
        "\n",
        "\n",
        "def create_mask(src,tgt):\n",
        "  src_seq_len=src.shape[0]\n",
        "  tgt_seq_len=tgt.shape[0]\n",
        "\n",
        "  tgt_mask=generate_square_subsequent_mask(tgt_seq_len)\n",
        "  src_mask=torch.zeros((src_seq_len,src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "  src_padding_mask=(src==PAD_IDX).transpose(0,1)\n",
        "  tgt_padding_mask=(tgt==PAD_IDX).transpose(0,1)\n",
        "  return src_mask,tgt_mask,src_padding_mask,tgt_padding_mask\n",
        "\n",
        "target_input=target_tensor[:-1,:]\n",
        "target_out=target_tensor[1:,:]\n",
        "\n",
        "source_mask,target_mask,source_padding_mask,target_padding_mask=create_mask(\n",
        "    source_tensor,target_input\n",
        ")\n",
        "\n",
        "print(\"source_mask:\",source_mask.shape)\n",
        "print(source_mask)\n",
        "\n",
        "print(\"target_mask:\",target_mask.shape)\n",
        "print(target_mask)\n",
        "\n",
        "print(\"source_padding_mask:\",source_padding_mask.shape)\n",
        "print(source_padding_mask)\n",
        "\n",
        "\n",
        "print(\"target_padding_mask:\",target_padding_mask.shape)\n",
        "print(target_padding_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "F_GdPc8q7FwA",
        "outputId": "599d43ac-027d-4946-942c-242f387da010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'target_tensor' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-5b32af471313>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt_padding_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtarget_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mtarget_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'target_tensor' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run(model,optimizer,driterion,split):\n",
        "  model.train() if split==\"train\"else model.eval()\n",
        "  data_iter=Multi30k(split=split,language_pair=(SRC_LANGUAGE,TGT_LANGUAGE))\n",
        "  dataloader=DataLoader(data_iter,batch_size=BATCH_SIZE,collate_fn=collator)\n",
        "\n",
        "  losses=0\n",
        "  for source_batch,target_batch in dataloader:\n",
        "    source_batch=source_batch.to(DEVICE)\n",
        "    target_batch=target_batch.to(DEVICE)\n",
        "\n",
        "    target_input=target_batch[:-1,:]\n",
        "    target_output=target_batch[1:,:]\n",
        "\n",
        "    src_mask,tgt_mask,src_padding_mask,tgt_padding_mask=create_mask(\n",
        "        source_batch,target_input\n",
        "    )\n",
        "\n",
        "    logits=model(\n",
        "        src=source_batch,\n",
        "        trg=target_input,\n",
        "        src_mask=src_mask,\n",
        "        tgt_mask=tgt_mask,\n",
        "        src_padding_mask=src_padding_mask,\n",
        "        tgt_padding_mask=tgt_padding_mask,\n",
        "        memory_key_padding_mask=src_padding_mask,\n",
        "    )\n",
        "    optimizer.zero_grad()\n",
        "    loss=criterion(logits.reshape(-1,logits.shape[-1]),target_output.reshape(-1))\n",
        "    if split==\"train\":\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    losses+=loss.item()\n",
        "  return losses/len(list(dataloader))\n",
        "\n",
        "for epoch in range(5):\n",
        "  train_loss=run(model,optimizer,criterion,\"train\")\n",
        "  val_loss=run(model,optimizer,criterion,\"valid\")\n",
        "  print(f\"Epoch:{epoch+1}, Train loss:{train_loss:.3f}, Val loss:{val_loss:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "3ZA8BQk48zpX",
        "outputId": "a238521c-4003-463b-acc1-5ea99c766c9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-f13f94e08734>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m   \u001b[0mtrain_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m   \u001b[0mval_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"valid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch:{epoch+1}, Train loss:{train_loss:.3f}, Val loss:{val_loss:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "그리드 디코딩\n",
        "\n",
        "각 단어별로 가장 높은 확률을 가지는 단어를 선택하는 방식으로 번역 진행\n",
        "\n"
      ],
      "metadata": {
        "id": "xO33K2nRG1Jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(model,source_tensor,source_mask,max_len,start_symbol):\n",
        "  source_tensor=source_tensor.to(DEVICE)\n",
        "  source_mask=source_mask.to(DEVICE)\n",
        "\n",
        "  memory=model.encode(source_tensor,source_mask)\n",
        "  ys=torch.ones(1,1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "  for i in range(max_len-1):\n",
        "    memory=memory.to(DEVICE)\n",
        "    target_mask=generate_square_subsequent_mask(ys.size(0))\n",
        "    target_mask=target_mask.type(torch.bool).to(DEVICE)\n",
        "\n",
        "    out=model.decode(ys,memory,target_mask)\n",
        "    out=out.transpose(0,1)\n",
        "    prob=model.generator(out[:,-1])\n",
        "    _,next_word=torch.max(prob,dim=1)\n",
        "    next_word=next_word.item()\n",
        "\n",
        "    ys=torch.cat(\n",
        "        [ys,torch.ones(1,1).type_as(source_tensor.data).fill_(next_word)],dim=0\n",
        "    )\n",
        "    if next_word==EOS_IDX:\n",
        "      break\n",
        "  return ys\n",
        "\n",
        "\n",
        "def translate(model,source_sentence):\n",
        "  model.eval()\n",
        "  source_tensor=text_transform[SRC_LANGUAGE](source_sentence).view(-1,1)\n",
        "  num_tokens=source_tensor.shape[0]\n",
        "  src_mask=(torch.zeros(num_tokens,num_tokens)).type(torch.bool)\n",
        "  tgt_tokens=greedy_decode(\n",
        "      model,source_tensor,src_mask,max_len=num_tokens+5,start_symbol=BOS_IDX\n",
        "  ).flatten()\n",
        "  output=vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))[1:-1]\n",
        "  return \" \".join(output)\n",
        "\n",
        "output_oov=translate(model,\"Eine Gruppe von Menschen steht vor einem Iglu.\")\n",
        "output=translate(model,\"Eine Gruppe vone Menschen steht vor einem Gebaude.\")\n",
        "print(output_oov)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "7x9vlQxyBvN-",
        "outputId": "10e7f40d-a5cc-4002-ec95-ccabb572a160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-18476f9d0d6c>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0moutput_oov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Eine Gruppe von Menschen steht vor einem Iglu.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Eine Gruppe vone Menschen steht vor einem Gebaude.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_oov\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dcSQ5vaoG0Uv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c8dR289rGiyX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}